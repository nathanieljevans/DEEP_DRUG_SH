{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Bayesian Neural Network\n",
    "\n",
    "### Quantifying uncertainty to improve prediction accuracy \n",
    "\n",
    "This model will be built using **only** beatAML data to predict AUC, however, this time we will use a bayesian neural network. \n",
    "\n",
    "Following this tutorial: https://forum.pyro.ai/t/dealing-with-noise-in-bayesian-neural-network-regression/863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "from torch.utils import data\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import imageio\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import colors\n",
    "from IPython import display\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical, Uniform\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "from   torch.distributions import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugExpressionDataset(Dataset): \n",
    "    '''\n",
    "    '''\n",
    "    def __init__ (self, label_dict, root_dir='../data_pytorch/tensors'): \n",
    "        '''\n",
    "        '''\n",
    "        self.index = list(label_dict.keys())\n",
    "        self.labels = label_dict\n",
    "        self.root = root_dir\n",
    "                            \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        '''\n",
    "        return len(self.labels.keys())\n",
    "        \n",
    "    def __getitem__(self, index, response_type=False):\n",
    "        '''\n",
    "        '''\n",
    "        fid = self.index[index]\n",
    "        X = torch.load(f'{self.root}/{fid}.pt')[:10, :] #<----------------------------- number of genes to use\n",
    "        _id, id_type, resp_type, response = self.labels[fid]\n",
    "        \n",
    "        if response_type: \n",
    "            return torch.as_tensor(X, dtype=torch.float), (response, resp_type)\n",
    "        else: \n",
    "            return torch.as_tensor(X, dtype=torch.float), response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate data into train/test/validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = (0.65,0.2,0.15)\n",
    "\n",
    "with open('../data_pytorch/label_dict.pkl', 'rb') as f: \n",
    "    label_dict = pickle.load(f)\n",
    "\n",
    "label_dict2 = dict()\n",
    "for fid in label_dict: \n",
    "    _id, id_type, resp_type, response = label_dict[fid]\n",
    "    \n",
    "    if resp_type not in label_dict2: \n",
    "        label_dict2[resp_type] = dict()\n",
    "        \n",
    "    p = np.random.rand()\n",
    "    if (p < train): # add to training set \n",
    "        if 'train' in label_dict2[resp_type]: \n",
    "            label_dict2[resp_type]['train'][fid] = (_id, id_type, resp_type, response)\n",
    "        else:\n",
    "            label_dict2[resp_type]['train'] = {fid:(_id, id_type, resp_type, response)}\n",
    "\n",
    "    elif (p < train+test): # add to test set \n",
    "        if 'test' in label_dict2[resp_type]: \n",
    "            label_dict2[resp_type]['test'][fid] = (_id, id_type, resp_type, response)\n",
    "        else:\n",
    "            label_dict2[resp_type]['test'] = {fid:(_id, id_type, resp_type, response)}\n",
    "    else: # add to validation set \n",
    "        if 'val' in label_dict2[resp_type]: \n",
    "            label_dict2[resp_type]['val'][fid] = (_id, id_type, resp_type, response)\n",
    "        else:\n",
    "            label_dict2[resp_type]['val'] = {fid:(_id, id_type, resp_type, response)}            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['RNAi_dependency', 'crispr_dependency', 'pooled_drugresp_prism', 'AUC_GDSC', 'CTRP_AUC', 'AUC_drug_CCLE', 'beatAML_AUC'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set sizes: RNAi_dependency      \t-> train  \t-> 186334\n",
      "set sizes: RNAi_dependency      \t-> val  \t-> 43031\n",
      "set sizes: RNAi_dependency      \t-> test  \t-> 57174\n",
      "set sizes: crispr_dependency    \t-> test  \t-> 62372\n",
      "set sizes: crispr_dependency    \t-> train  \t-> 202161\n",
      "set sizes: crispr_dependency    \t-> val  \t-> 46435\n",
      "set sizes: pooled_drugresp_prism \t-> test  \t-> 30491\n",
      "set sizes: pooled_drugresp_prism \t-> train  \t-> 99099\n",
      "set sizes: pooled_drugresp_prism \t-> val  \t-> 22837\n",
      "set sizes: AUC_GDSC             \t-> train  \t-> 10812\n",
      "set sizes: AUC_GDSC             \t-> test  \t-> 3382\n",
      "set sizes: AUC_GDSC             \t-> val  \t-> 2470\n",
      "set sizes: CTRP_AUC             \t-> train  \t-> 17944\n",
      "set sizes: CTRP_AUC             \t-> val  \t-> 4166\n",
      "set sizes: CTRP_AUC             \t-> test  \t-> 5520\n",
      "set sizes: AUC_drug_CCLE        \t-> train  \t-> 1462\n",
      "set sizes: AUC_drug_CCLE        \t-> test  \t-> 470\n",
      "set sizes: AUC_drug_CCLE        \t-> val  \t-> 351\n",
      "set sizes: beatAML_AUC          \t-> train  \t-> 1136\n",
      "set sizes: beatAML_AUC          \t-> val  \t-> 266\n",
      "set sizes: beatAML_AUC          \t-> test  \t-> 347\n"
     ]
    }
   ],
   "source": [
    "for resp_type in label_dict2:\n",
    "    for sset in label_dict2[resp_type]: \n",
    "        pp = resp_type + ' '*(20 - len(resp_type))\n",
    "        print(f'set sizes: {pp} \\t-> {sset}  \\t-> {len(label_dict2[resp_type][sset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "798259"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(label_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN_SVI:\n",
    "    def __init__(self, dim, H1, H2, Hout, conf = dict()):\n",
    "        self.dim           = dim\n",
    "        self.print_every   = conf.get('print_every',  100)\n",
    "        self.lr            = conf.get('lr',           1e-3)\n",
    "        self.weight_prior  = conf.get('weight_prior', 1.0)\n",
    "        self.bias_prior    = conf.get('bias_prior',   1.0)\n",
    "        self.prec_alpha    = conf.get('prec_alpha', 3)\n",
    "        self.prec_beta     = conf.get('prec_beta', 1)\n",
    "        self.nn = nn.Sequential(\n",
    "                nn.Linear(self.dim, H1), nn.Tanh(),\n",
    "                nn.Linear(H1, Hout))\n",
    "        self.prior_variance = 1\n",
    "\n",
    "    def model(self, X, y):\n",
    "        #noise_scale = torch.tensor(noise_level)\n",
    "        #precision   = pyro.sample(\"precision\", pyro.distributions.Gamma(self.prec_alpha, self.prec_beta))\n",
    "        #noise_scale = 1 / precision.sqrt()\n",
    "        noise_scale = pyro.sample('noise_scale_unif', pyro.distributions.Uniform(0,10) )\n",
    "        num_x       = X.shape[0]\n",
    "        priors      = dict()\n",
    "        for n, p in self.nn.named_parameters():\n",
    "            if \"weight\" in n:\n",
    "                priors[n] = pyro.distributions.Normal(\n",
    "                        loc   = torch.zeros_like(p),\n",
    "                        scale = self.prior_variance * torch.ones_like(p)).to_event(1)\n",
    "            elif \"bias\" in n:\n",
    "                priors[n] = pyro.distributions.Normal(\n",
    "                        loc   = torch.zeros_like(p),\n",
    "                        scale = self.prior_variance * torch.ones_like(p)).to_event(1)\n",
    "\n",
    "        lifted_module    = pyro.random_module(\"module\", self.nn, priors)\n",
    "        lifted_reg_model = lifted_module()\n",
    "        with pyro.plate(\"map\", len(X), subsample_size = num_x) as ind:\n",
    "            prediction_mean = lifted_reg_model(X[ind]).squeeze(-1)\n",
    "            pyro.sample(\"obs\", \n",
    "                    pyro.distributions.Normal(prediction_mean, noise_scale), \n",
    "                    obs = y[ind])\n",
    "\n",
    "    def guide(self, X, y):\n",
    "        softplus  = nn.Softplus()\n",
    "        alpha     = pyro.param(\"alpha\", torch.tensor(self.prec_alpha), constraint = constraints.positive)\n",
    "        beta      = pyro.param(\"beta\",  torch.tensor(self.prec_beta),  constraint = constraints.positive)\n",
    "        precision = pyro.sample(\"precision\", pyro.distributions.Gamma(alpha, beta))\n",
    "        \n",
    "        priors      = dict()\n",
    "        for n, p in self.nn.named_parameters():\n",
    "            if \"weight\" in n:\n",
    "                loc   = pyro.param(\"mu_\"    + n, self.weight_prior * torch.randn_like(p))\n",
    "                scale = pyro.param(\"sigma_\" + n, softplus(torch.randn_like(p)), constraint = constraints.positive)\n",
    "                priors[n] = pyro.distributions.Normal(loc = loc, scale = self.prior_variance * scale).to_event(1)\n",
    "            elif \"bias\" in n:\n",
    "                loc       = pyro.param(\"mu_\"    + n, self.bias_prior * torch.randn_like(p))\n",
    "                scale     = pyro.param(\"sigma_\" + n, softplus(torch.randn_like(p)), constraint = constraints.positive)\n",
    "                priors[n] = pyro.distributions.Normal(loc = loc, scale = self.prior_variance * scale).to_event(1)\n",
    "        lifted_module = pyro.random_module(\"module\", self.nn, priors)\n",
    "        return lifted_module()\n",
    "            \n",
    "    def train(self, data_gen, max_epoch, lr=None):\n",
    "        if lr is not None: \n",
    "            thislr = lr\n",
    "        else: \n",
    "            thislr = self.lr\n",
    "        self.expr_mean, self.expr_std, self.y_mean, self.y_std = [0]*4\n",
    "        for X,y in data_gen:\n",
    "            self.y_mean += y.size(0)*y.mean() / len(data_gen.dataset)\n",
    "            self.y_std  += y.size(0)*y.std() / len(data_gen.dataset)\n",
    "            self.expr_mean += X.size(0)*X[:,:,0].mean(0) / len(data_gen.dataset)\n",
    "            self.expr_std += X.size(0)*X[:,:,0].std(0) / len(data_gen.dataset)\n",
    "\n",
    "        optim       = pyro.optim.Adam({\"lr\":thislr})\n",
    "        svi         = pyro.infer.SVI(self.model, self.guide, optim, loss = Trace_ELBO() )\n",
    "        pyro.clear_param_store()\n",
    "        self.rec = []\n",
    "        for i in range(max_epoch):\n",
    "            for X,y in data_gen:\n",
    "                X[:,:,0] = (X[:,:,0] - self.expr_mean) / self.expr_std\n",
    "                X = X.reshape(X.size(0), -1)\n",
    "                y = (y - self.y_mean) / self.y_std\n",
    "                #y.unsqueeze(1)\n",
    "                loss = svi.step(X, y)\n",
    "                self.rec.append(loss / len(data_gen.dataset))\n",
    "            if (i+1) % self.print_every == 0:\n",
    "                print(\"[Iteration %05d] loss: %.4f \\t\\t\\t\" % (i + 1, loss / len(data_gen.dataset)), end='\\r')\n",
    "    \n",
    "    def sample_predict(self, x, n=10):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        x[:,:,0] = (x[:,:,0] - self.expr_mean) / self.expr_std\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        sampled_models = [self.guide(None, None) for _ in range(n)]\n",
    "        yhats = [(model(x.view(-1,self.dim).data)* self.y_std + self.y_mean).detach().numpy() for model in sampled_models]\n",
    "        return np.asarray(yhats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Initialize our model and DataLoader(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN ### \n",
    "\n",
    "params = {'batch_size': 3*128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "train_set = DrugExpressionDataset(label_dict2['beatAML_AUC']['train'])\n",
    "\n",
    "train_gen = data.DataLoader(train_set, **params)\n",
    "\n",
    "### TEST ### \n",
    "\n",
    "params2 = {'batch_size': 1,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 0}\n",
    "\n",
    "test_set = DrugExpressionDataset(label_dict2['beatAML_AUC']['test'])\n",
    "test_gen = data.DataLoader(test_set, **params2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 00010] loss: 48.3272 \t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "NGENES = 10\n",
    "\n",
    "conf                 = dict()\n",
    "conf['print_every']  = 1\n",
    "conf['weight_priro'] = 1#/np.sqrt(NGENES)\n",
    "conf['bias_priro']   = 1#/np.sqrt(NGENES)\n",
    "conf['lr']           = 1e-2\n",
    "conf['prec_alpha']   = 1. # precision mean = 20 variance = 40\n",
    "conf['prec_beta']    = 1.\n",
    "\n",
    "model = BNN_SVI(dim = NGENES*2, H1 = 500, H2= 100, Hout = 1, conf = conf)\n",
    "model.train(train_gen, max_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting...9/347\r"
     ]
    }
   ],
   "source": [
    "ys = []\n",
    "yhats = []\n",
    "ii = 0\n",
    "for x,y in test_gen: \n",
    "    print(f'predicting...{ii}/{len(test_gen.dataset)}', end='\\r')\n",
    "    yhat = model.sample_predict(x, n=10)\n",
    "    ys += y.data.numpy().tolist()\n",
    "    yhats.append(yhat)\n",
    "    ii += 1\n",
    "    if ii == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yhat_mu = [np.mean(x) for x in yhats]\n",
    "yhat_std = [np.std(x) for x in yhats]\n",
    "df = pd.DataFrame({'y':ys,'yhat_mu':yhat_mu, 'yhat_std':yhat_std})\n",
    "df.sort_values(by='y', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(df['y'], 'ro', label='y [true]')\n",
    "plt.plot(df['yhat_mu'], 'bo', label = 'yhat [predicted]')\n",
    "plt.plot(df['yhat_mu'] + 2*df['yhat_std'], 'g--', label = '2*std yhat [predicted uncertainty]')\n",
    "plt.plot(df['yhat_mu'] - 2*df['yhat_std'], 'g--')\n",
    "[plt.plot(x,y,'b.', alpha=0.2) for x,y in zip(yhats,range(len(yhats)))]\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recorder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b2a7eff0c5fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean squared error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recorder' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(np.log10(recorder['train']['mse']), 'r--', label='train mse')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Model training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_total_loss = 0\n",
    "yhats = []\n",
    "ys = []\n",
    "for X, y in test_gen:\n",
    "    X,y = X.to(device, dtype=torch.float), y.to(device, dtype=torch.float)\n",
    "    y = y.unsqueeze(1)\n",
    "    yhat = net.forward(X)\n",
    "    test_total_loss += loss_func(yhat, y).detach().numpy()\n",
    "    yhats += yhat.data.numpy().ravel().tolist()\n",
    "    ys += y.data.numpy().ravel().tolist()\n",
    "\n",
    "net.train()\n",
    "\n",
    "print(f'test mse: {test_total_loss/len(test_gen.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y':ys, 'yhat':yhats}).assign(diff = lambda x: np.max(x.y) * (x.y-x.yhat)**2 / np.max((x.y-x.yhat)**2))\n",
    "\n",
    "df.sort_values(by='y', inplace=True)\n",
    "\n",
    "df = df.assign(diff_rm = df['diff'].rolling(window=10).mean())\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(df['y'].values, 'ro', label='y [true]')\n",
    "plt.plot(df['yhat'].values, 'bo', label='yhat [predicted]')\n",
    "plt.plot(df['diff_rm'].values, 'g--', label='scaled rolling mean quadratic error')\n",
    "plt.xlabel('Sorted Observations')\n",
    "plt.ylabel('Area Under the Curve (AUC)')\n",
    "plt.title(f'BeatAML FCNN Test Set Predictions [n={len(test_gen.dataset)}]')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

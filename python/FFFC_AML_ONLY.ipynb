{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Baseline \n",
    "\n",
    "This model will be built using **only** beatAML data to predict AUC. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle\n",
    "from torch.utils import data\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and show learning process\n",
    "#from : https://medium.com/@benjamin.phillips22/simple-regression-with-neural-networks-in-pytorch-313f06910379\n",
    "\n",
    "def plot_learning_process(axes, tr_ys, tr_yhats, tst_ys, tst_yhats, epoch, tr_loss, tst_loss):\n",
    "\n",
    "    axes[0].cla()\n",
    "    \n",
    "    ######### TRAIN #########\n",
    "    tr_df = pd.DataFrame({'y':tr_ys, 'yhat':tr_yhats})\n",
    "    tr_df.sort_values(by='y', inplace=True)\n",
    "    \n",
    "    axes[0].plot(tr_df.values[:,0], 'ro', label='true', alpha=0.5)\n",
    "    axes[0].plot(tr_df.values[:,1], 'bo', label='predicted', alpha=0.5)\n",
    "    #plt.legend()\n",
    "    \n",
    "    axes[0].set_title('Regression Analysis [Training Set]', fontsize=15)\n",
    "    axes[0].set_xlabel('Sorted observations', fontsize=24)\n",
    "    axes[0].set_ylabel('AUC', fontsize=24)\n",
    "\n",
    "    axes[0].text(100, 30, 'Epoch = %d' % epoch, fontdict={'size': 24, 'color':  'red'})\n",
    "    axes[0].text(100, 50, 'Loss = %.4f' % loss, fontdict={'size': 24, 'color':  'red'})\n",
    "\n",
    "    ######### TEST #########\n",
    "    \n",
    "    axes[1].cla()\n",
    "    \n",
    "    tst_df = pd.DataFrame({'y':tst_ys, 'yhat':tst_yhats})\n",
    "    tst_df.sort_values(by='y', inplace=True)\n",
    "    \n",
    "    axes[1].plot(tst_df.values[:,0], 'ro', label='true', alpha=0.5)\n",
    "    axes[1].plot(tst_df.values[:,1], 'bo', label='predicted', alpha=0.5)\n",
    "    plt.legend()\n",
    "    \n",
    "    axes[1].set_title('Regression Analysis [Validation Set]', fontsize=15)\n",
    "    axes[1].set_xlabel('Sorted observations', fontsize=24)\n",
    "    axes[1].set_ylabel('AUC', fontsize=24)\n",
    "\n",
    "    axes[1].text(100, 30, 'Epoch = %d' % epoch, fontdict={'size': 24, 'color':  'red'})\n",
    "    axes[1].text(100, 50, 'Loss = %.4f' % loss, fontdict={'size': 24, 'color':  'red'})\n",
    "\n",
    "    ######## DA REST #############\n",
    "    \n",
    "    # Used to return the plot as an image array \n",
    "    # (https://ndres.me/post/matplotlib-animated-gifs-easily/)\n",
    "    fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugExpressionDataset(Dataset): \n",
    "    '''\n",
    "    '''\n",
    "    def __init__ (self, label_dict, root_dir='../data_pytorch/tensors'): \n",
    "        '''\n",
    "        '''\n",
    "        self.index = list(label_dict.keys())\n",
    "        self.labels = label_dict\n",
    "        self.root = root_dir\n",
    "                            \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        '''\n",
    "        return len(self.labels.keys())\n",
    "        \n",
    "    def __getitem__(self, index, response_type=False):\n",
    "        '''\n",
    "        '''\n",
    "        fid = self.index[index]\n",
    "        X = torch.load(f'{self.root}/{fid}.pt') #<----------------------------- number of genes to use\n",
    "        _id, id_type, resp_type, response = self.labels[fid]\n",
    "        \n",
    "        if response_type: \n",
    "            return X, (response, resp_type)\n",
    "        else: \n",
    "            return X, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate data into train/test/validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = (0.65,0.2,0.15)\n",
    "\n",
    "with open('../data_pytorch/label_dict.pkl', 'rb') as f: \n",
    "    label_dict = pickle.load(f)\n",
    "\n",
    "label_dict2 = dict()\n",
    "for fid in label_dict: \n",
    "    _id, id_type, resp_type, response = label_dict[fid]\n",
    "    \n",
    "    if resp_type not in label_dict2: \n",
    "        label_dict2[resp_type] = dict()\n",
    "        \n",
    "    p = np.random.rand()\n",
    "    if (p < train): # add to training set \n",
    "        if 'train' in label_dict2[resp_type]: \n",
    "            label_dict2[resp_type]['train'][fid] = (_id, id_type, resp_type, response)\n",
    "        else:\n",
    "            label_dict2[resp_type]['train'] = {fid:(_id, id_type, resp_type, response)}\n",
    "\n",
    "    elif (p < train+test): # add to test set \n",
    "        if 'test' in label_dict2[resp_type]: \n",
    "            label_dict2[resp_type]['test'][fid] = (_id, id_type, resp_type, response)\n",
    "        else:\n",
    "            label_dict2[resp_type]['test'] = {fid:(_id, id_type, resp_type, response)}\n",
    "    else: # add to validation set \n",
    "        if 'val' in label_dict2[resp_type]: \n",
    "            label_dict2[resp_type]['val'][fid] = (_id, id_type, resp_type, response)\n",
    "        else:\n",
    "            label_dict2[resp_type]['val'] = {fid:(_id, id_type, resp_type, response)}            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['RNAi_dependency', 'crispr_dependency', 'pooled_drugresp_prism', 'AUC_GDSC', 'CTRP_AUC', 'AUC_drug_CCLE', 'beatAML_AUC'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set sizes: RNAi_dependency      \t-> train  \t-> 186441\n",
      "set sizes: RNAi_dependency      \t-> test  \t-> 57075\n",
      "set sizes: RNAi_dependency      \t-> val  \t-> 43023\n",
      "set sizes: crispr_dependency    \t-> train  \t-> 202084\n",
      "set sizes: crispr_dependency    \t-> val  \t-> 46587\n",
      "set sizes: crispr_dependency    \t-> test  \t-> 62297\n",
      "set sizes: pooled_drugresp_prism \t-> train  \t-> 99007\n",
      "set sizes: pooled_drugresp_prism \t-> test  \t-> 30561\n",
      "set sizes: pooled_drugresp_prism \t-> val  \t-> 22859\n",
      "set sizes: AUC_GDSC             \t-> train  \t-> 10808\n",
      "set sizes: AUC_GDSC             \t-> test  \t-> 3347\n",
      "set sizes: AUC_GDSC             \t-> val  \t-> 2509\n",
      "set sizes: CTRP_AUC             \t-> test  \t-> 5608\n",
      "set sizes: CTRP_AUC             \t-> train  \t-> 17855\n",
      "set sizes: CTRP_AUC             \t-> val  \t-> 4167\n",
      "set sizes: AUC_drug_CCLE        \t-> train  \t-> 1485\n",
      "set sizes: AUC_drug_CCLE        \t-> test  \t-> 445\n",
      "set sizes: AUC_drug_CCLE        \t-> val  \t-> 353\n",
      "set sizes: beatAML_AUC          \t-> train  \t-> 1129\n",
      "set sizes: beatAML_AUC          \t-> val  \t-> 251\n",
      "set sizes: beatAML_AUC          \t-> test  \t-> 369\n"
     ]
    }
   ],
   "source": [
    "for resp_type in label_dict2:\n",
    "    for sset in label_dict2[resp_type]: \n",
    "        pp = resp_type + ' '*(20 - len(resp_type))\n",
    "        print(f'set sizes: {pp} \\t-> {sset}  \\t-> {len(label_dict2[resp_type][sset])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "798259"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(label_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our model architecture \n",
    "\n",
    "Going to use a straight forward 4 layer FFFC network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, ngenes, sz=(250,100,10), do=0.3, nconvs = 5):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        H1, H2, H3 = sz\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(1, nconvs, kernel_size=(1,2), stride=1, padding=0),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.do1 = torch.nn.Dropout(do)\n",
    "        self.fc1 = nn.Linear(ngenes*nconvs, H1)\n",
    "        self.bn1 = nn.BatchNorm1d(H1)\n",
    "        self.do2 = torch.nn.Dropout(do)\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.bn2 = nn.BatchNorm1d(H2)\n",
    "        self.do3 = torch.nn.Dropout(do)\n",
    "        self.fc3 = nn.Linear(H2, H3)\n",
    "        self.bn3 = nn.BatchNorm1d(H3)\n",
    "        self.out = nn.Linear(H3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        z0 = self.layer1(x)\n",
    "        z0 = z0.reshape(z0.size(0), -1)\n",
    "        \n",
    "        z1 = self.fc1( z0 )\n",
    "        z1 = self.do1( z1 )\n",
    "        z1 = self.bn1( z1 )\n",
    "        a1 = F.relu( z1 )\n",
    "        \n",
    "        z2 = self.do2( self.fc2(a1) )\n",
    "        z2 = self.bn2( z2 )\n",
    "        a2 = F.relu(z2)\n",
    "        \n",
    "        z3 = self.do3( self.fc3(a2) )\n",
    "        z3 = self.bn3( z3 )\n",
    "        a3 = F.relu(z3)\n",
    "        \n",
    "        o = self.out(a3)\n",
    "        \n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Net(nn.Module):\\n\\n    def __init__(self, ngenes, sz=(250,100,10), do=0.3, nconvs = 5):\\n        super(Net, self).__init__()\\n        \\n        H1, H2, H3 = sz\\n      \\n        self.fc1 = nn.Linear(ngenes*2, H1)\\n        self.fc2 = nn.Linear(H1, H2)\\n        self.fc3 = nn.Linear(H2, H3)\\n        self.out = nn.Linear(H3, 1)\\n\\n    def forward(self, x):\\n        x = x.reshape(x.size(0), -1)\\n        \\n        z1 = self.fc1( x )\\n        a1 = F.leaky_relu( z1 )\\n        \\n        z2 = self.fc2(a1) \\n        a2 = F.leaky_relu(z2)\\n        \\n        z3 = self.fc3(a2) \\n        a3 = F.leaky_relu(z3)\\n        \\n        o = self.out(a3)\\n        \\n        return o\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, ngenes, sz=(250,100,10), do=0.3, nconvs = 5):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        H1, H2, H3 = sz\n",
    "      \n",
    "        self.fc1 = nn.Linear(ngenes*2, H1)\n",
    "        self.fc2 = nn.Linear(H1, H2)\n",
    "        self.fc3 = nn.Linear(H2, H3)\n",
    "        self.out = nn.Linear(H3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        z1 = self.fc1( x )\n",
    "        a1 = F.leaky_relu( z1 )\n",
    "        \n",
    "        z2 = self.fc2(a1) \n",
    "        a2 = F.leaky_relu(z2)\n",
    "        \n",
    "        z3 = self.fc3(a2) \n",
    "        a3 = F.leaky_relu(z3)\n",
    "        \n",
    "        o = self.out(a3)\n",
    "        \n",
    "        return o\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Initialize our model and DataLoader(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_WEIGHT = 1e-2\n",
    "EPOCHS = 250\n",
    "DROP_OUT_PROP = 0.1\n",
    "LAYER_SIZE= (1000, 1000, 500) \n",
    "NCONVS = 2\n",
    "WEIGHT_DECAY = 0.01\n",
    "NGENES = 523 #/523\n",
    "\n",
    "# Reproducibility\n",
    "#torch.manual_seed(2)\n",
    "#np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 2, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (do1): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=1046, out_features=4000, bias=True)\n",
      "  (bn1): BatchNorm1d(4000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (do2): Dropout(p=0.1, inplace=False)\n",
      "  (fc2): Linear(in_features=4000, out_features=2000, bias=True)\n",
      "  (bn2): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (do3): Dropout(p=0.1, inplace=False)\n",
      "  (fc3): Linear(in_features=2000, out_features=1000, bias=True)\n",
      "  (bn3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=1000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(ngenes = NGENES, sz=LAYER_SIZE, do=DROP_OUT_PROP, nconvs=NCONVS)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 128*2,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "train_set = DrugExpressionDataset(label_dict2['beatAML_AUC']['train'])\n",
    "test_set = DrugExpressionDataset(label_dict2['beatAML_AUC']['test'])\n",
    "val_set = DrugExpressionDataset(label_dict2['beatAML_AUC']['val'])\n",
    "\n",
    "train_gen = data.DataLoader(train_set, **params)\n",
    "test_gen = data.DataLoader(test_set, **params)\n",
    "val_gen = data.DataLoader(val_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters(recurse=True), lr=LEARNING_WEIGHT, weight_decay=WEIGHT_DECAY)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)  #torch.optim.lr_scheduler.MultiStepLR(optim, milestones=DECAY_MILESTONES, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 226/250 \t|\t train total loss: 12998.36 \t train mse: 11.51 \t|\t val mse: 12.28 \t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "recorder={'train':{'total_loss':[], 'mse':[]}, 'val':{'mse':[]}}\n",
    "images = []\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,7))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    mse = 0\n",
    "    yhats=[]\n",
    "    ys=[]\n",
    "    for X,y in train_gen:\n",
    "        y = y.unsqueeze(1)\n",
    "        X,y = X.to(device, dtype=torch.float), y.to(device, dtype=torch.float)\n",
    "        yhat = net(X)\n",
    "        loss = loss_func(yhat, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step() \n",
    "        scheduler.step(loss)\n",
    "        total_loss += loss.detach().numpy()\n",
    "        yhats += yhat.data.numpy().ravel().tolist()\n",
    "        ys += y.data.numpy().ravel().tolist()\n",
    "        \n",
    "    recorder['train']['total_loss'].append(total_loss)\n",
    "    recorder['train']['mse'].append(total_loss/len(train_gen.dataset))\n",
    "\n",
    "    val_total_loss = 0\n",
    "    val_yhats=[]\n",
    "    val_ys=[]\n",
    "    net.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for X, y in val_gen:\n",
    "            X,y = X.to(device, dtype=torch.float), y.to(device, dtype=torch.float)\n",
    "            y = y.unsqueeze(1)\n",
    "            yhat = net(X)\n",
    "            val_total_loss += loss_func(yhat, y).data.numpy()\n",
    "            val_yhats += yhat.data.numpy().ravel().tolist()\n",
    "            val_ys += y.data.numpy().ravel().tolist()\n",
    "        recorder['val']['mse'].append(val_total_loss / len(val_gen.dataset))\n",
    "    \n",
    "    images.append( plot_learning_process(axes=ax, tr_ys=ys , tr_yhats=yhats, tst_ys = val_ys, tst_yhats = val_yhats, epoch=epoch, tr_loss=total_loss/len(train_gen.dataset), tst_loss=val_total_loss / len(val_gen.dataset)) )\n",
    "    print(f'epoch {epoch+1}/{EPOCHS} \\t|\\t train total loss: {total_loss:.2f} \\t train mse: {total_loss/len(train_gen.dataset):.2f} \\t|\\t val mse: {val_total_loss / len(val_gen.dataset):.2f} \\t\\t\\t', end='\\r')\n",
    "    \n",
    "imageio.mimsave('../FCNN_training.gif', images, fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(recorder['train']['mse'], 'r--', label='train mse')\n",
    "plt.plot(recorder['val']['mse'], 'b--', label='val mse')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title('Model training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test our model\n",
    "WHY IS THIS GIVING SLIGHTLY DIFFERENT VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "test_total_loss = 0\n",
    "yhats = []\n",
    "ys = []\n",
    "for X, y in test_gen:\n",
    "    X,y = X.to(device, dtype=torch.float), y.to(device, dtype=torch.float)\n",
    "    y = y.unsqueeze(1)\n",
    "    yhat = net.forward(X)\n",
    "    test_total_loss += loss_func(yhat, y).detach().numpy()\n",
    "    yhats += yhat.detach().numpy().ravel().tolist()\n",
    "    ys += y.detach().numpy().ravel().tolist()\n",
    "\n",
    "net.train()\n",
    "\n",
    "print(f'test mse: {test_total_loss/len(test_gen.dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y':ys, 'yhat':yhats})\n",
    "df.sort_values(by='y', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(df.values[:,0], 'ro', label='y')\n",
    "plt.plot(df.values[:,1], 'bo', label='yhat')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
